import os
import torch as th
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.utils import set_random_seed
from src.config import bootstrap
from src.environment import TrafficSignalEnv

# Ø¯Ø§Ù„Ø© Ù„ØªØºÙŠÙŠØ± Ø³Ø±Ø¹Ø© Ø§Ù„ØªØ¹Ù„Ù… ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹ (Smart Scheduler)
def linear_schedule(initial_value: float):
    def func(progress_remaining: float) -> float:
        return progress_remaining * initial_value
    return func

def main():
    # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
    cfg, log, device = bootstrap("configs/config.yaml")
    
    # ØªØ«Ø¨ÙŠØª Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù†ØªØ§Ø¦Ø¬ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙƒØ±Ø§Ø±
    set_random_seed(42)

    # 2. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
    rl_models_dir = os.path.join("models", "rl_agents")
    os.makedirs(rl_models_dir, exist_ok=True)
    
    # **Ù…Ù‡Ù…:** Ø§Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¨Ø§Ø³Ù… Ø¢Ø®Ø± Ø¹Ø´Ø§Ù† Ù…Ø§ ÙŠØ¶ÙŠØ¹!
    if os.path.exists(os.path.join(rl_models_dir, "final_ppo_agent.zip")):
        os.rename(
            os.path.join(rl_models_dir, "final_ppo_agent.zip"),
            os.path.join(rl_models_dir, "final_ppo_agent_OLD_1560.zip")
        )

    # 3. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙŠØ¦Ø©
    log.info("Creating Traffic Signal Environment (Pro Mode)...")
    env = TrafficSignalEnv(cfg)

    # 4. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© "Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©" (Deep Network)
    # [256, 256] ØªØ¹Ù†ÙŠ Ø·Ø¨Ù‚ØªÙŠÙ† ÙƒÙ„ ÙˆØ§Ø­Ø¯Ø© ÙÙŠÙ‡Ø§ 256 Ø¹ØµØ¨ ØµÙ†Ø§Ø¹ÙŠ (Ø¯Ù…Ø§Øº Ø£ÙƒØ¨Ø± Ø¨Ù€ 4 Ø£Ø¶Ø¹Ø§Ù)
    policy_kwargs = dict(
        activation_fn=th.nn.Tanh,
        net_arch=dict(pi=[256, 256], vf=[256, 256])
    )

    # 5. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ø­ØªØ±Ø§ÙÙŠØ©
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=linear_schedule(0.0003),
        gamma=0.99,
        n_steps=4096,
        batch_size=128,
        policy_kwargs=policy_kwargs,
        tensorboard_log=cfg["paths"]["log_dir"],
        device="cpu"
    )

    # 6. Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø·ÙˆÙŠÙ„ (500,000 Ø®Ø·ÙˆØ©)
    # Ø³ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø£Ø·ÙˆÙ„ (20-40 Ø¯Ù‚ÙŠÙ‚Ø©) Ù„ÙƒÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø© ØªØ³ØªØ§Ù‡Ù„!
    total_steps = 500000 
    log.info(f"ğŸš€ Starting PRO Training for {total_steps} steps...")
    
    checkpoint_callback = CheckpointCallback(
        save_freq=50000, 
        save_path=rl_models_dir, 
        name_prefix="ppo_pro"
    )

    model.learn(total_timesteps=total_steps, callback=checkpoint_callback)

    # 7. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    final_path = os.path.join(rl_models_dir, "final_ppo_agent")
    model.save(final_path)
    log.info(f"ğŸ† Training Finished! Super-Model saved to {final_path}")

if __name__ == "__main__":
    main()