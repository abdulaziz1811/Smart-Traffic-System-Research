import torch
from transformers import DetrImageProcessor, DetrForObjectDetection, Trainer, TrainingArguments
from torchvision.datasets import CocoDetection
import os

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø²
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
print(f"ğŸ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬: {device}")

# 2. Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
img_folder = 'data/raw/DETRAC-Images/DETRAC-Images'
ann_file = 'data/annotations/train.json'

# 3. ØªØ¹Ø±ÙŠÙ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
id2label = {1: "Car", 2: "Bus", 3: "Van", 4: "Others"}
label2id = {"Car": 1, "Bus": 2, "Van": 3, "Others": 4}

# 4. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬ (Ù†Ø³Ø®Ø© Lite)
print("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ù†Ø³Ø®Ø© 'Lite' Ù„ØªÙ†Ø§Ø³Ø¨ M2...")

processor = DetrImageProcessor.from_pretrained(
    "facebook/detr-resnet-50",
    do_resize=True,
    size={"shortest_edge": 480, "longest_edge": 640}, # ØªØµØºÙŠØ± Ø§Ù„ØµÙˆØ± Ù‡Ùˆ Ø§Ù„Ø­Ù„ Ø§Ù„Ø³Ø­Ø±ÙŠ
    image_mean=[0.485, 0.456, 0.406],
    image_std=[0.229, 0.224, 0.225]
)

model = DetrForObjectDetection.from_pretrained(
    "facebook/detr-resnet-50",
    num_labels=len(id2label),
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True
)
model.to(device)

# 5. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
class DetrDataset(CocoDetection):
    def __init__(self, img_folder, ann_file, processor):
        super(DetrDataset, self).__init__(img_folder, ann_file)
        self.processor = processor

    def __getitem__(self, idx):
        img, target = super(DetrDataset, self).__getitem__(idx)
        image_id = self.ids[idx]
        target = {'image_id': image_id, 'annotations': target}
        encoding = self.processor(images=img, annotations=target, return_tensors="pt")
        pixel_values = encoding["pixel_values"].squeeze()
        target = encoding["labels"][0]
        return {"pixel_values": pixel_values, "labels": target}

train_dataset = DetrDataset(img_folder, ann_file, processor)

# 6. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø®ØµØµØ© Ù„Ù„Ù…Ø§Ùƒ (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ø±Ø©)
training_args = TrainingArguments(
    output_dir="models/weights/detr_finetuned",
    per_device_train_batch_size=1,   # Ø¯ÙØ¹Ø© ØµØºÙŠØ±Ø©
    gradient_accumulation_steps=8,   # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª
    num_train_epochs=1,
    save_steps=500,
    logging_steps=50,
    learning_rate=1e-4,
    weight_decay=1e-4,
    save_total_limit=2,
    remove_unused_columns=False,
    report_to="none",
    dataloader_pin_memory=False,
    gradient_checkpointing=False,    # [ØªÙ… Ø§Ù„ØªØ¹Ø·ÙŠÙ„] Ù„ØªÙØ§Ø¯ÙŠ Ø§Ù„Ø®Ø·Ø£
    dataloader_num_workers=0
)

def collate_fn(batch):
    pixel_values = [item["pixel_values"] for item in batch]
    encoding = processor.pad(pixel_values, return_tensors="pt")
    labels = [item["labels"] for item in batch]
    return {
        "pixel_values": encoding["pixel_values"],
        "pixel_mask": encoding["pixel_mask"],
        "labels": labels
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=collate_fn,
)

print("ğŸš€ Ø§Ù†Ø·Ù„Ø§Ù‚ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø®ÙÙ (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© Ø«Ø§Ø¨ØªØ©!)...")
trainer.train()

# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
model.save_pretrained("models/weights/final_model")
processor.save_pretrained("models/weights/final_model")
print("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡!")