import torch
from transformers import DetrImageProcessor, DetrForObjectDetection
from PIL import Image, ImageDraw, ImageFont
import os
import cv2
import numpy as np
from tqdm import tqdm

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø² ÙˆØ§Ù„Ù…ÙˆØ¯ÙŠÙ„
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
print(f"ğŸ Using device: {device}")

model_path = "models/weights/final_model"
# Ø³Ù†Ø®ØªØ§Ø± Ù…Ø¬Ù„Ø¯Ø§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ Ù„Ù„ØªØ¬Ø±Ø¨Ø© (ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± Ø§Ù„Ø±Ù‚Ù… Ù„Ù…Ø¬Ù„Ø¯ Ø¢Ø®Ø± Ù…Ø«Ù„ MVI_40131)
# Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¨Ø­Ø« Ø¹Ù† Ø£ÙˆÙ„ Ù…Ø¬Ù„Ø¯ ÙŠØ¬Ø¯Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
base_dir = 'data/raw/DETRAC-Images/DETRAC-Images'
available_dirs = [d for d in os.listdir(base_dir) if d.startswith("MVI")]
if not available_dirs:
    print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ù„Ø¯Ø§Øª MVI ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±!")
    exit()

# Ù†Ø®ØªØ§Ø± Ù…Ø¬Ù„Ø¯ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø£Ùˆ Ù…Ø­Ø¯Ø¯ (Ù…Ø«Ù„Ø§Ù‹ MVI_40131 Ø§Ù„Ù…Ø´Ù‡ÙˆØ±)
sequence_name = available_dirs[0] 
image_folder = os.path.join(base_dir, sequence_name)
print(f"ğŸ¬ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ø¬Ù„Ø¯: {sequence_name}")

# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
try:
    processor = DetrImageProcessor.from_pretrained(model_path)
    model = DetrForObjectDetection.from_pretrained(model_path)
    model.to(device)
    model.eval()
except:
    print("âŒ ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø³Ø§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„!")
    exit()

# 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
output_video_path = f"traffic_analysis_{sequence_name}.mp4"
images = sorted([img for img in os.listdir(image_folder) if img.endswith(".jpg")])

if not images:
    print("âŒ Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙØ§Ø±Øº!")
    exit()

# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
first_frame = cv2.imread(os.path.join(image_folder, images[0]))
height, width, layers = first_frame.shape
fps = 25 # Ø³Ø±Ø¹Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ

# Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙˆØ¯Ùƒ Ù„Ø¶ØºØ· Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (mp4v ÙŠØ¹Ù…Ù„ Ø¬ÙŠØ¯Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø§Ùƒ)
fourcc = cv2.VideoWriter_fourcc(*'mp4v') 
video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

# 4. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ø­Ø¯Ø© ØªÙ„Ùˆ Ø§Ù„Ø£Ø®Ø±Ù‰
print(f"ğŸš€ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(images)} Ø¥Ø·Ø§Ø±... (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚)")

for img_name in tqdm(images):
    img_path = os.path.join(image_folder, img_name)
    
    # ÙØªØ­ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù€ PIL Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„
    pil_image = Image.open(img_path).convert("RGB")
    
    # Ø§Ù„ØªÙ†Ø¨Ø¤
    with torch.no_grad():
        inputs = processor(images=pil_image, return_tensors="pt").to(device)
        outputs = model(**inputs)
        
    target_sizes = torch.tensor([pil_image.size[::-1]]).to(device)
    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.5)[0]

    # Ø§Ù„Ø±Ø³Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenCV (Ø£Ø³Ø±Ø¹ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ)
    # Ù†Ø­ÙˆÙ„ Ù…Ù† PIL Ø¥Ù„Ù‰ OpenCV
    opencv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

    for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
        box = [int(i) for i in box.tolist()]
        label_name = model.config.id2label[label.item()]
        conf = round(score.item(), 2)

        # Ø£Ù„ÙˆØ§Ù† Ù…Ø®ØªÙ„ÙØ©
        color = (0, 0, 255) # Ø£Ø­Ù…Ø± Ù„Ù„Ø³ÙŠØ§Ø±Ø§Øª (BGR)
        if label_name == "Bus": color = (255, 0, 0) # Ø£Ø²Ø±Ù‚
        if label_name == "Van": color = (0, 255, 0) # Ø£Ø®Ø¶Ø±

        # Ø±Ø³Ù… Ø§Ù„Ù…Ø±Ø¨Ø¹
        cv2.rectangle(opencv_image, (box[0], box[1]), (box[2], box[3]), color, 2)
        
        # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ
        label_text = f"{label_name} {conf}"
        cv2.putText(opencv_image, label_text, (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¥Ø·Ø§Ø± Ù„Ù„ÙÙŠØ¯ÙŠÙˆ
    video.write(opencv_image)

# 5. Ø¥Ù†Ù‡Ø§Ø¡ ÙˆØ­ÙØ¸
video.release()
cv2.destroyAllWindows()
print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø¬Ø§Ø­: {output_video_path}")
os.system(f"open {output_video_path}")